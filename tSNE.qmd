## Stochastic Neighbor Embedding (SNE) \scriptsize [@hinton2002stochastic]

#### High dimensional space

Let $(\bx_1, \hdots, \bx_n)$ be the original points in $\mathbb{R}^p$, and measure similarities by

$$
p_{ij} =  (p_{j | {i}} + p_{{i} | j})/ 2n, \quad \text{with } p_{j | {i}} = \frac{ \exp(- \| \bx_j - \bx_{i} \|^2 / 2 \sigma_i^2 ) }{\sum_{k \neq i} \exp(- \| \bx_k - \bx_{i} \|^2 / 2 \sigma_{i}^2)}
$$

  - preserves relations with \emphase{close neighbors}
  - $\sigma_i$ adjusts to local densities (neighborhood of $i$)

#### Perplexity

A smoothed effective number of neighbors:
$$
  Perp(p_i) = 2^{H(p_i)}, \qquad H(p_i) = -\sum_{j=1}^{n} p_{j|i} \log_2 p_{j|i}
$$

\rsa $\sigma_i$ found by binary search to match a user-defined perplexity for $p_i$

## tSNE and Student / Cauchy kernels \scriptsize [@MH08]

#### Similarities in the low dimension space

Let $(\bz_1,\hdots,\bz_n)$ be the points in the the low-dimensional space $\mathbb{R}^{q=2}$

$$\begin{aligned}
\emphase{(SNE)} & \quad q_{i | j} = \frac{ \exp(- \| \bz_i - \bz_j \|^2  ) }{\sum_{k \neq i} \exp(- \| \bz_k - \bz_j \|^2 )} \\
\emphase{(t-SNE)} & \quad q_{i | j} = \frac{ (1 + \| \bz_i - \bz_j \|^2)^{-1}  }{\sum_{k \neq i} (1 + \| \bz_i - \bz_k \|^2)^{-1}} \\
\end{aligned}$$

 \rsa t-SNE robustifies Gaussian kernel by using Student(1) (Cauchy) kernels

#### Optimization

\alert{Criterion} -- Kullback-Leibler between $p$ and $q:$ $C(\bz) = \sum_{ij} KL(p_{ij},q_{ij})$ \smallskip

\alert{Algorithm} -- adaptive stochastic gradient initialized by $\mathcal{N}(0,\epsilon I_q)$ \smallskip

\alert{Initiatization} -- reduce original data with PCA then initialized by $\mathcal{N}(0,\epsilon I_q)$ \smallskip
<!-- $$
Z^{(t)} = Z^{(t-1)} + \eta \frac{\partial C(Z)}{\partial Z} + \alpha(t) (Z^{(t-1)}-Z^{(t-2)})
$$ -->

## Empirical properties of tSNE (1)

![](figs/pne/tsne_properties_1.pdf) 

## Empirical properties of tSNE (2)

![](figs/pne/tsne_properties_2.pdf) 

## Empirical properties of tSNE (3)

![](figs/pne/tsne_properties_3.pdf) 

<!-- 
## t-SNE: pros/cons

#### Properties

  - good at preserving local distances (intra-cluster variance)
  - not so good for global representation (inter-cluster variance)
  - good at creating clusters of close points, bad at positioning clusters wrt each other

#### Limitations

  - importance of preprocessing: initialize with PCA and feature selection plus log transform (non linear transform)
  - percent of explained variance ? interpretation of the $q$ distribution ?
  - Lack of reproducibility due to stochastic optimization


## Uniform Manifold Approximation and Projection \linebreak \scriptsize @IHM18

For $j$ in the $k$-neighborhood of $i$, define the conditional distribution
$$
p_{j \mid i} = \exp \left(-\frac{\| X_i-X_j \|^2_2 - \rho_i}{\sigma_i}\right) \quad \text{ with } \rho_i = \min_{j\neq i} \| X_i-X_j \|^2
$$ 

and its symmetrized version

\vspace{-.25cm}

$$
p_{ij}  = p_{ j | i} + p_{ i | j} - p_{ j | i} p_{ i | j}.
$$

Rely on a generalized Student-distribution with $a, b$ fitted on the data:

\vspace{-.25cm}

$$
q_{ij} =  \left(1 + a\| Z_i-Z_j \|^{2b}_2\right)^{-1}
$$

UMAP solves the following problem:

\vspace{-.25cm}
$$
\min_{Z\in \mathbb{R}^{n \times d}} - \sum_{i < j} p_{ij}\log q_{ij} + (1-p_{ij}) \log (1- q_{ij})
$$

\emphase{Tends to preserve both local and global representations}
 -->
