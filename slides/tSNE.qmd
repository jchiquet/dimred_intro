## Stochastic Neighbor Embedding (SNE)

Let $(\bx_1, \hdots, \bx_n)$ be the original points in $\mathbb{R}^p$, and measure similarities by

$$
p_{ij} =  (p_{j | {i}} + p_{{i} | j})/ 2n
$$

where

$$\begin{aligned}
  p_{j | {i}} & = \frac{ \exp(- \| \bx_j - \bx_{i} \|^2 / 2 \sigma_i^2 ) }{\sum_{k \neq i} \exp(- \| \bx_k - \bx_{i} \|^2 / 2 \sigma_{i}^2)}, \\
  & = \frac{ \exp(- d_{ij}^2 / 2 \sigma_i^2 ) }{\sum_{k \neq i} \exp(- d_{ki}^2 / 2 \sigma_i^2)}
\end{aligned}$$


  - SNE preserves relations with \alert{\bf close neighbors} with Gaussian kernels
  - $\sigma$ smooths the data (linked to the regularity of the target manifold)

## The perplexity parameter

The variance $\sigma_i^2$ should adjust to local densities (neighborhood of point $i$)

#### Perplexity: a smoothed effective number of neighbors

The perplexity is defined by
$$
  Perp(p_i) = 2^{H(p_i)}, \qquad H(p_i) = -\sum_{j=1}^{n} p_{j|i} \log_2 p_{j|i}
$$
where $H$ is the Shannon entropy of $p_i=(p_{1|i},\hdots,p_{n|i})$.\\

\rsa SNE performs a binary search for the value of $\sigma_i$ that produces a $p_i$ with a fixed perplexity that is specified by the user.

## tSNE and Student / Cauchy kernels

Consider $(\tilde{\bx}_1,\hdots,\tilde{\bx}_n)$ are points in the low dimensional space $\mathbb{R}^{q=2}$

  - Consider a similarity between points in the new representation:
$$q_{i | j} = \frac{ \exp(- \| \tilde{\bx}_i - \tilde{\bx}_j \|^2  ) }{\sum_{k \neq i} \exp(- \| \tilde{\bx}_k - \tilde{\bx}_j \|^2 )}$$
  - Robustify this kernel by using Student(1) kernels (ie Cauchy)
$$q_{i | j} = \frac{ (1 + \| \tilde{\bx}_i - \tilde{\bx}_j \|^2)^{-1}  }{\sum_{k \neq i} (1 + \| \tilde{\bx}_i - \tilde{\bx}_k \|^2)^{-1}}$$

## Optimizing tSNE

  - Minimize the KL between $p$ and $q$ so that the data representation minimizes:
$$
C(y) = \sum_{ij} KL(p_{ij},q_{ij})
$$
  - The cost function is not convex 
$$
\left[ \frac{\partial C(y)}{\partial y} \right]_i = \sum_{j} (p_{ij}-q_{ij})(y_i - y_j)
$$

  - Gradient update (adaptive learning rate $\eta$) with momentum $\alpha(t)$
$$
Z^{(t)} = Z^{(t-1)} + \eta \frac{\partial C(Z)}{\partial Z} + \alpha(t) (Z^{(t-1)}-Z^{(t-2)})
$$
  - Initialization $Z_i^{(0)} \sim \mathcal{N}(0,\delta I)$, $\delta$ small.


## Empirical properties of tSNE (1)

![](figs_PNE/tsne_properties_1.pdf) 

## Empirical properties of tSNE (2)

![](figs_PNE/tsne_properties_2.pdf) 

## Empirical properties of tSNE (3)

![](figs_PNE/tsne_properties_3.pdf) 

## tSNE on single cell Gene Expression data @kobak_tsne_2018

![](figs_PNE/tsne_10xgenomics.pdf) 

## t-SNE: pros/cons

#### Properties

  - good at preserving local distances (intra-cluster variance)
  - not so good for global representation (inter-cluster variance)
  - good at creating clusters of close points, bad at positioning clusters wrt each other

#### Limitations

  - importance of preprocessing: initialize with PCA and feature selection plus log transform (non linear transform)
  - percent of explained variance ? interpretation of the $q$ distribution ?
  - Lack of reproducibility due to stochastic optimization


## Uniform Manifold Approximation and Projection [@IHM18]

For $j$ in the $k$-neighborhood of $i$, define the conditional distribution
$$
p_{j \mid i} = \exp \left(-\frac{\| X_i-X_j \|^2_2 - \rho_i}{\sigma_i}\right) \quad \text{ with } \rho_i = \min_{j\neq i} \| X_i-X_j \|^2
$$ 

and its symmetrized version

$$
p_{ij}  = p_{ j | i} + p_{ i | j} - p_{ j | i} p_{ i | j}.
$$

Rely on a generalized Student-distribution with $a, b$ fitted on the data:

$$
q_{ij} =  \left(1 + a\| Z_i-Z_j \|^{2b}_2\right)^{-1}
$$

UMAP solves the following problem:

$$\begin{aligned}
\min_{Z\in \mathbb{R}^{n \times d}} \quad & - \sum_{i < j} p_{ij}\log q_{ij} + (1-p_{ij}) \log (1- q_{ij})
\end{aligned}$$

\emphase{Tends to preserve both local and global representations}

