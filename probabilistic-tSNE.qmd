
## Probabilistic Neighborhood Embedding \scriptsize [@van2022probabilistic]


\framesubtitle{Unifying probabilistic view of t-SNE like approaches}

<!-- ^[Another important reference: @cai2022theoretical (Analysis of the algorithm, not model-based)] -->

#### Hidden graph to structure observations

Let $\bW$ describe a hidden random graph with Laplacian $\bL = \bD - \bW$

<!-- ^[with one connected component, without loss of generality] and its Laplacian  with $\bD_{ii} = \text{deg}(i)$. -->

<!-- $$ -->
<!-- \sum_{i,j} W_{ij}\|\bX_i - \bX_j\|^2 = \operatorname{tr}(\bX^T \bL \bX). -->
<!-- $$ -->

#### Conditional distribution structured by graphs

Consider matrix Normal models with \emphase{row} and column dependencies

$$
\bX \mid \bW_X  \sim \mathcal{MN} \bigg(0 , \bL_X^{-1},\, \bSigma^{-1} \bigg), \quad \bZ \mid \bW_Z  \sim \mathcal{MN} \bigg(0 , \bL_Z^{-1},\, \mathbf{I}_q\bigg).
$$

The conditional densities relates to the Gaussian kernel:

\vspace{-.5cm}

$$
k ( \bX_i - \bX_j) = \exp \left( - \frac{1}{2} \| \bX_i-\bX_{j} \|^2_{\Sigma^{-1}} \right), \, k(\bZ_i - \bZ_j) = \exp \left( - \frac{1}{2} \| \bZ_i-\bZ_j \|^2 \right).
$$
which can be generalized to translation invariant kernels:

\vspace{-.5cm}

$$
\mathbb{P}(\bX \mid \bW_X) \varpropto \prod_{(i,j)} k(\bX_{i} - \bX_{j})^{W_{X,ij}},
\mathbb{P}(\bZ \mid \bW_Z) \varpropto \prod_{(i,j)} k(\bZ_{i} - \bZ_{j})^{W_{Z,ij}}.
$$

## Embedding with Graph Coupling

:::: {.columns}

::: {.column width="60%"}
Couple the hidden graphs $W_X$ and $W_Z$ in by matching their posterior distributions:

$$\mathbf{P}^{X} = \mathbb{P}(W_X \mid \bX), \quad \mathbf{Q}^{Z} = \mathbb{P}(W_Z \mid \bX; \bZ)$$

$\rightsquigarrow$ $Z$ becomes a parameter to be estimated

:::

::: {.column width="40%"}

![](figs/pne/two_graph_model_W.pdf){width="70%"}

:::

::::

#### Optimize cross-entropy between posteriors
 
$$
\mathcal{H}\big(\mathbf{P}^{X}, \mathbf{Q}^{Z} \big) = - \mathbb{E}_{W_X \sim \mathbf{P}^{X}} \bigg(\log \mathbb{P}(W_Z=W_X \mid X ; Z) \bigg)
$$

Find the best low-dimensional representation such that the 2 graphs match

$$
\hat{\bZ}(X) = \argmin_{\bZ} \mathcal{H}\big(\mathbf{P}^{X}, \mathbf{Q}^{Z} \big) = \argmin_{\bZ} D_\text{KL}\big(\mathbf{P}^{X}, \mathbf{Q}^{Z} \big) 
$$

## Conjugate priors and posteriors for hidden graphs

Consider a prior distribution for the hidden graph in the general form
$$\mathbb{P}_{\mathcal{P}}(\mathbf{W}; {\boldsymbol\pi}) \, \propto \, \underbrace{\cancel{\mathcal{C}_k(W)^\alpha}}_{\alpha=0} \, \Omega_{\mathcal{P}}(\bW) \prod_{(i,j) \in [n]^2} \pi_{ij}^{W_{ij}}$$

For the following priors family, we derive the posterior $\mathbb{P}_{\mathcal{P}}(\mathbf{W}\mid X ; {\boldsymbol\pi}, k)$

\begin{center}
\begin{footnotesize}
\begin{tabular}{l|ccc}
$\mathcal{P}$           & $\Omega_{\mathcal{P}}(\bW)$                  & Prior for $W$ & $\approx$ Posterior for $W$\\
\hline
$\mathcal{B}\quad$ Bernoulli                &  $\prod_{ij} \mathbf{1}_{W_{ij} \leq 1}$   & $\mathcal{B}\left( \frac{\pi_{ij}}{ 1 + \pi_{ij} } \right)$&  $\mathcal{B}\left( \frac{\pi_{ij}k_{ij}}{1 + \pi_{ij}k_{ij}} \right)$\\
$\mathcal{D}\quad$ Unitary Fixed degree           & $\prod_{i} \mathbf{1}_{W_{i+} = 1}$        & $\mathcal{M}\left(1, \frac{{\boldsymbol\pi}_{i}}{\pi_{i+}} \right)$ & $\mathcal{M}\left(1, \frac{[{\pi k}]_i}{[\pi k]_{i+}} \right)$ \\
$\mathcal{E}\quad$ Fixed Number of edges & $\prod_{ij}(W_{ij}!)^{-1}$                  & $\mathcal{M}\left(n, \frac{{\boldsymbol\pi}}{\pi_{++}} \right)$ &  $\mathcal{M}\left(n, \frac{{\pi k}}{ [\pi k]_{++}}\right)$\\
\end{tabular}
$\pi_{ij}k_{ij} = \pi_{ij}k(X_i-X_j)$ is the posterior strength of edges (normalized or not)
\end{footnotesize}
\end{center}

\bigskip

#### Mixing Prior distributions for coupling

Priors for $W_X, W_Z$ induce posteriors $\mathbf{P}^{X}, \mathbf{Q}^{Z}$ matched with cross entropy $\mathcal{H}\big(\mathbf{P}^{X},\mathbf{Q}^{Z}\big)$

## Model-based Neighbor Embedding

Choosing $\mathcal{P}_X=\mathcal{P}_Z=\mathcal{D}$ lead us to $\displaystyle \mathcal{H}_{D,D} = - \sum_{i \neq j} P^{D}_{ij} \log Q^{D}_{ij} \:$ and

$$\begin{aligned}
P^{D}_{ij} = \frac{\pi_{ij}k(X_i-X_j)}{\sum_{\ell = 1}^n \pi_{i\ell}k(X_i-X_\ell)}, \quad 
Q^{D}_{ij} = \frac{\pi_{ij}k(Z_i-Z_j)}{\sum_{\ell = 1}^n \pi_{i\ell}k(Z_i-Z_\ell)}.
\end{aligned}$$

\emphase{We defined the generative model for SNE!}. Similarly,

\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}llll@{}}
Algorithm & Input Similarity & Latent Similarity & Loss Function  \\ [0.4em]
\hline &&&\\[-0.7em]
SNE & $P_{ij}^{D} = \frac{k_x(\mathbf{X}_{i} - \mathbf{X}_{j})}{\sum_\ell k_x(\mathbf{X}_{i} - \mathbf{X}_{\ell})}$ & $Q_{ij}^{D} = \frac{k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}{\sum_\ell k_z(\mathbf{Z}_{i} - \mathbf{Z}_{\ell})}$ & $- \sum_{i \neq j} P^{D}_{ij} \log Q^{D}_{ij}$ \\ [0.8em]
\hline &&&\\[-0.7em]
Sym-SNE & $\overline{P}^{D}_{ij} = P_{ij}^{D} + P_{ji}^{D}$ & $Q_{ij}^{E} = \frac{k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}{\sum_{\ell,t} k_z(\mathbf{Z}_{\ell} - \mathbf{Z}_{t})}$ & $- \sum_{i < j} \overline{P}^{D}_{ij} \log 
Q^{E}_{ij}$ \\ [0.8em]
\hline &&&\\[-0.7em]
LargeVis & $\overline{P}^{D}_{ij} = P_{ij}^{D} + P_{ji}^{D}$ & $Q^{B}_{ij} = \frac{k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}{1+k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}$ & $- \sum_{i < j} \overline{P}^{D}_{ij} \log Q^{B}_{ij} + \left(2-\overline{P}^{D}_{ij}\right) \log (1- Q^{B}_{ij})$ \\ [0.8em]
\hline &&&\\[-0.7em]
UMAP & $\widetilde{P}^{B}_{ij} = P^{B}_{ij} + P^{B}_{ji} - P^{B}_{ij}P^{B}_{ji}$ & $Q^{B}_{ij} = \frac{k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}{1+k_z(\mathbf{Z}_{i} - \mathbf{Z}_{j})}$ & $- \sum_{i < j} \widetilde{P}^{B}_{ij} \log Q^{B}_{ij} + \left(1-\widetilde{P}^{B}_{ij}\right) \log (1- Q^{B}_{ij})$ \\ [0.8em]
\end{tabular}
\end{scriptsize}
\end{center}

<!-- ## Model-based UMAP

Choose $\mathcal{P}_X=\mathcal{P}_Z=\mathcal{B}$ and define the symmetrized graph

$$
\widetilde{\bW}_{X} = \mathbf{1}_{\bW_{X} + \bW_X^T \geq 1}
$$

By independence of the symmetrized edges, 

$$ 
\widetilde{W}_{X,ij} \sim \mathcal{B}\left(\widetilde{P}^{B}_{ij} \right)
\quad \text{with} \quad
\widetilde{P}^{B}_{ij} = P^{B}_{ij} + P^{B}_{ji} - P^{B}_{ij} P^{B}_{ji}
$$

Coupling $\widetilde{\bW}_{X}$ and $\bW_{Z}$ gives

$$
\mathcal{H}_{\widetilde{B},B} = -2 \sum_{i<j} \widetilde{P}_{ij}^{B} \log Q_{ij}^{B} + \left(1 - \widetilde{P}_{ij}^{B} \right) \log \left( 1 - Q_{ij}^{B} \right)
$$ -->

