## Reconstruction error approach

  1. Construct a map $\Phi$ from the space $\Rset^{p}$ into a space $\Rset^{q}$ of \alert{smaller dimension}:

$$\begin{aligned}
  \Phi:\quad & \Rset^{p} \to \Rset^{q}, q \ll p\\
             & \bx \mapsto \Phi(\bx) = \tilde\bx
\end{aligned}$$

  2. Construct $\tilde{\Phi}$ from $\Rset^{q}$ to $\Rset^{p}$ (\alert{reconstruction formula})
  3. Control an error $\epsilon$ between $\bx$ and its reconstruction $\hat \bx = \tilde{\Phi}(\Phi(\bx))$


For instance,  the error measured with the Frobenius between the original data matrix $\bX$ and its approximation:

$$
\epsilon(\bX, \hat \bX ) = \left\| \bX - \hat \bX \right\|_F^2  = \sum_{i=1}^n \left\| \bx_i - \tilde{\Phi}(\Phi(\bx_i)) \right\|^2 
$$

## Reinterpretation of PCA

#### PCA model

Let $\bV$ be a $p\times q$ matrix whose columns are of $q$ orthonormal vectors.

$$\begin{aligned}
  \Phi(\bx) & = \bV^\top(\bx-\bmu)  = \tilde\bx \\  
  \bx \simeq \tilde{\Phi}(\tilde\bx) & = \bmu + \bV \tilde\bx
\end{aligned}$$

\rsa Model with \emphase{Linear assumption + ortho-normality constraints}
 
#### PCA reconstruction error

$$
\minimize_{\bmu \in\Rset^p, \bV\in\mathcal{M}_{p,q}} \sum_{i=1}^n \left\| (\bx_i  - \bmu) - \bV\bV^\top ( \bx_i -\bmu)   \right\|^2
$$
  
\paragraph{Solution (explicit)} 
  
  - $\bmu = \bar{\bx}$ the empirical mean
  - $\bV$  an orthonormal basis of the space spanned by the $q$ first eigenvectors of the empirical covariance matrix

## Important digression: SVD

#### Singular Value Decomposition (SVD)

The SVD of $\mathbf{M}$ a $n\times p$ matrix is the factorization given by

$$
  \mathbf{M} =\mathbf{U}\mathbf{D}\mathbf{V}^\top,
$$

where $r = \min(n,p)$ and
    
  - $\mathbf{D}_{r \times r} = \text{diag}(\delta_1, ...\delta_r)$ is the diagonal matrix of singular values.
  - $\mathbf{U}$ is orthonormal, whose columns are eigen vectors of ($\mathbf{M}\mathbf{M}^T$)
  - $\mathbf{V}$ is orthonormal whose columns are eigen vectors of ($\mathbf{M}^T\mathbf{M}$)
 
    \rsa Time complexity in $\mathcal{O}(n p q r)$ (less when $k\ll r$ components are required)
  
  \vfill
  
#### Connection with eigen decomposition of the covariance matrix

$$\begin{aligned}
  \mathbf{M}^\top\mathbf{M} & = \mathbf{V} \mathbf{D} \mathbf{U}^\top  \mathbf{U} \mathbf{D} \mathbf{V}^\top \\
& = \mathbf{V} \mathbf{D}^2 \mathbf{V}^\top  = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top\\
\end{aligned}$$

## PCA solution is given by SVD of the centered data matrix

![](figs_common/matrix_factorization.pdf){height=4cm}

Since $\tilde\bX = \mathbf{\bX}^c \bV =  \bU \bD \bV^\top \bV = \bU \bD$, PCA can be rephrased as

$$\hat{\mathbf{X}^c} = \mathbf{FV}^\top =  \argmin_{\mathbf{F}\in\mathcal{M}_{n,q},\bV\in\mathcal{O}_{p,q} } \left\| \mathbf{X}^c - \mathbf{FV}^\top \right\|_F^2 \text{ with } \|\mathbf{A}\|_F^2 = \sum_{ij} a_{ij}^2, 
$$

$$
  \left. \tilde\bX \in\Rset^{n\times \textcolor{red}{q}}, \mathbf{V}\in\Rset^{p\times \textcolor{red}{q}} \right\} \ \text{Best linear low-rank representation of $\bX$}
$$
 
