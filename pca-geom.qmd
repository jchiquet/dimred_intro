
## Cloud of observation in $\Rset^p$ and Inertia

\framesubtitle{Individuals in the \alert{variable space $\Rset^p$}}

:::: {.columns}

::: {.column width="65%"}

Cloud $\bX$ is centered around^[empirical mean, barycentrum, center of inertia] $\bar{\bx}=\sum _{i=1}^n \bx_i / n$ 

\begin{footnotesize}
\begin{equation*}
  \bX^c = \begin{pmatrix}
  x_{11} - \bar{x}_1 &   \dots & x_{1j}  - \bar{x}_j & \dots  & x_{1p} - \bar{x}_p   \cr
            \vdots   &  \vdots & \vdots              & \vdots & \vdots  \cr
  x_{i1} - \bar{x}_1 &   \dots & x_{ij} - \bar{x}_j  & \dots  & x_{ip}  - \bar{x}_p \cr
            \vdots   &  \vdots & \vdots              & \vdots & \vdots  \cr
  x_{n1} - \bar{x}_1 &  \dots  & x_{nj} - \bar{x}_j  & \dots  & x_{np}  - \bar{x}_p \cr
  \end{pmatrix}
\end{equation*}
\end{footnotesize}

:::

::: {.column width="30%"}

![Example in $\Rset^3$](figs_pca/cloud_centering.jpg){width="85%"}

:::

::::

#### Total Inertia $I_T$ as a measure of information

Distances to the center of the cloud $\varpropto$ the total empirical variance
$$
I_T = \frac{1}{n}\sum_{i=1}^n \sum_{j=1}^p  (x_{ij}- \bar{x}_{j}) ^2 = \frac{1}{n}\sum_{i=1}^n \distance^2 (\bx_i,\bar{\bx}) = \sum_{j=1}^p \var(\bx^j) = \trace{\hat{\bSigma}}
$$

\rsa \alert{Good representation has large inertia} (much variability)

## Decomposition of total Inertia and best axis

#### Cloud inertia with respect to an axis

Sum of distances between all points and their orthogonal projection on $\Delta$.

$$
I_\Delta = \frac{1}{n}\sum_{i=1}^n \distance^2(\bx_i, \Delta)
$$

::: {#thm-huygens}

## Huygens, by Pythagoras

Let $\Delta^\bot$ be the orthogonal subspace of $\Delta$, then $I_T = I_{\Delta} + I_{\Delta^\bot}$. 
:::

\rsa \alert{By projecting the cloud $\bX$ onto $\Delta$, we loss the inertia measured by $\Delta^\bot$}

#### Best axis $\Delta_1$

The "closest" to the cloud, \textit{ie} \emphase{with minimal inertia}, or equivalently 

$$
 \Delta_1 =  \argmin_{\Delta} I_{\Delta} \Leftrightarrow \argmax_{\Delta^\bot} I_{\Delta^\bot}
$$

## Finding the best axis

Let $\bv$ be the unitary director vector of $\Delta$, normal to $\Delta^\bot$.

:::: {.columns}

::: {.column width="60%"}

$$\begin{aligned}
I_{\Delta^\bot} & = \frac{1}{n}\sum_{i=1}^n \distance(\bx_i,\Delta^\bot)^2 \\ 
& = \frac{1}{n}\sum_{i=1}^n \bv^\top (\bx_i - \bar{\bx})(\bx_i - \bar{\bx})^\top \bv \\
& = \bv^\top \hat{\bSigma}  \bv \\
\end{aligned}$$

:::

::: {.column width="35%"}

![](figs_pca/solving_inertia.jpg)

:::

::::

\begin{equation*}
  \argmax_{\Delta} I_{\Delta^\bot} \Leftrightarrow \argmax_{\bv_1: \| \bv_1 \| = 1 } \bv_1^\top \hat{\bSigma} \bv_1 
\end{equation*}

  \rsa $\Delta_1$ is defined by $\bv_1$, the \emphase{first eigen vector} of $\hat{\bSigma}$

  \rsa Variance "carried" by $\Delta_1$ is equal to the \emphase{largest eigen value} $\lambda_1$  of $\hat{\bSigma}$

## Finding the following axes

Consider collection of orthogonal axes (with dimension =1), then
\begin{equation*}
  I_{T} = I_{\Delta_1} + I_{\Delta_2} + \dots + I_{\Delta_p}
\end{equation*}

#### PCA is matrix factorisation

\vspace{-.25cm}

$$\hat{\bSigma} = \bV {\boldsymbol\Lambda} \bV^\top, \quad
    \bV = \begin{pmatrix}
    \bv_1 & \bv_2, & \dots & \bv_p
    \end{pmatrix}, \quad {\boldsymbol\Lambda} = \diag(\lambda_1, \dots, \lambda_p)
$$

$\bV$ are known as the \emphase{loadings}

#### Interpretation in $\Rset^p$
    
$\bV$ describes a new orthogonal basis and a rotation of data in this basis
  
:::: {.columns}

::: {.column}

\begin{equation*}
  \left\{\begin{array}{ccccc}
    \Delta_1 & \oplus & \dots & \oplus & \Delta_p \\
    \bv_1 & \bot & \dots & \bot & \bv_p \\
    \lambda_1 & > & \dots & > & \lambda_p \\
    I_{\Delta_1^\bot} & > & \dots & > & I_{\Delta_p^\bot} \\
  \end{array}\right.
\end{equation*}

:::

::: {.column width="25%"}

![](figs_pca/rotation.jpg){width="100%"}

:::

::: {.column width="25%"}

![](figs_pca/PCA_fish.png){width="100%"}

:::

:::: 

$\rightsquigarrow$ PCA is an appropriate rotation on axes that maximizes the variance

## Unifying view of variables and individuals

In the new basis $\{\bv_1, \dots, \bv_p\}$, coordinates of $i$ (a.k.a. \emphase{scores}) are

\vspace{-.25cm}

\begin{equation*}
  \bc_i^\top  = (\bx_i - \bar{\bx})^\top \bV = \bX^c_i \bV, \quad \bc_i \in \Rset^p.
\end{equation*}

In the variable space $\Rset^n$, new variables (factors) are formed by linear combinations of the orginal variables: the \emphase{principal components} (PC)

\vspace{-.25cm}

\begin{equation*}
  \mathbf{f}_{k}  = \sum_{j=1}^p v_{kj} (\bx^{j} - \bar{x}_j) = \bX^c \bv_k, \quad \mathbf{f}_k \in \Rset^n
\end{equation*}

The matrix of PC connects individual coordinates to latent factors:

\vspace{-.25cm}

\begin{footnotesize}
\begin{equation*}
    \mathrm{PC} = \bX^c \bV = \begin{pmatrix}
    \mathbf{f}_{1} & \mathbf{f}_{2} & \dots & \mathbf{f}_{p}
    \end{pmatrix}
    = \begin{pmatrix} 
    \bc_{1}^\top \\ \bc_{2}^\top \\\dots \\ \bc_{n}^\top 
    \end{pmatrix}
\end{equation*}
\end{footnotesize}

$\rightsquigarrow$ Everything can be interpreted on a single plot, called the biplot

