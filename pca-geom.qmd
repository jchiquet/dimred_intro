# Geometric approach to PCA

## Cloud of observation in $\Rset^p$

Individuals can be represented in the \alert{variable space $\Rset^p$} as a point cloud

:::: {.columns}

::: {.column width="45%"}

![Example in $\Rset^3$](figs_pca/cloud_centering.jpg){width="60%"}

:::

::: {.column width="45%"}

#### Center of Inertia

(barycentrum, empirical mean)

\begin{equation*}
\bar{\bx} = \frac{1}{n} \sum _{i=1}^n \bx_i
\end{equation*}

:::

::::

We center the cloud $\bX$ around $\bx$ denote this by $\bX^c$

\begin{footnotesize}
\begin{equation*}
  \bX^c = \begin{pmatrix}
  x_{11} - \bar{x}_1 &   \dots & x_{1j}  - \bar{x}_j & \dots  & x_{1p} - \bar{x}_p   \cr
            \vdots   &  \vdots & \vdots              & \vdots & \vdots  \cr
  x_{i1} - \bar{x}_1 &   \dots & x_{ij} - \bar{x}_j  & \dots  & x_{ip}  - \bar{x}_p \cr
            \vdots   &  \vdots & \vdots              & \vdots & \vdots  \cr
  x_{n1} - \bar{x}_1 &  \dots  & x_{nj} - \bar{x}_j  & \dots  & x_{np}  - \bar{x}_p \cr
  \end{pmatrix}
\end{equation*}
\end{footnotesize}

## Inertia and Variance

\paragraph{Total Inertia:} distance of the individuals to the center of the cloud

\begin{equation*}
    I_T = \frac{1}{n}\sum_{i=1}^n \sum_{j=1}^p  (x_{ij}- \bar{x}_{j}) ^2
    = \frac{1}{n}\sum_{i=1}^n \|\bx_i - \bar{\bx} \|^2
    = \frac{1}{n}\sum_{i=1}^n \distance^2 (\bx_i,\bar{\bx})
\end {equation*}

#### Proportional to the total variance

Let $\hat{\bSigma}$ be the empirical variance-covariance matrix
\begin{equation*}
      I_T = \frac{1}{n}\sum_{j=1}^p  \sum_{i=1}^n (x_{ij}- \bar{x}_{j}) ^2
      = \sum_{j=1}^p \frac{1}{n}\|\bx^j - \bar{x}_{j} \|^2
      = \sum_{j=1}^p \var(\bx^j) = \trace{\hat{\bSigma}}
\end{equation*}

$\rightsquigarrow$ \alert{Good representation has large inertia} (much variability)

$\rightsquigarrow$ \alert{Large dispertion $\sim$ Large distances between points}

## Inertia with respect to an axis

The Inertia of the cloud wrt axe $\Delta$ is the sum of the distances between all points and their orthogonal projection on $\Delta$.

\begin{equation*}
  \begin{aligned}
    I_\Delta = \frac{1}{n}\sum_{i=1}^n \distance^2(\bx_i, \Delta)
    \end{aligned}
\end{equation*}

![Projection of $\bx_i$ onto a line $\Delta$ passing through $\bar\bx$](figs_pca/proj_axis.jpg){width="60%"}

## Decomposition of total Inertia (1)

Let $\Delta^\bot$ be the orthogonal subspace of $\Delta$ in $\Rset^p$

![](figs_pca/supp_spaces.jpg){width="50%"}

::: {#thm-huygens}

## Huygens

A consequence of the above (Pythagoras Theorem) is the decomposition of the following total inertia:

\begin{equation*}
  I_T = I_{\Delta} + I_{\Delta^\bot}
\end{equation*}

\alert{By projecting the cloud $\bX$ onto $\Delta$, with loss the inertia measured by $\Delta^\bot$}

:::

## Decomposition of total Inertia (2)

Consider only subspaces with dimension $1$ (that is, lines or axes). We can decompose $\Rset^p$ as the sum of $p$ othogonal axis.

\begin{equation*}
  \Rset^p = \Delta_1 \oplus \Delta_2 \oplus \dots \oplus \Delta_p
\end{equation*}

\alert{$\rightsquigarrow$ These axes form a new basis for representing the point cloud.}

::: {#thm-huygens2}

## Huygens

\begin{equation*}
  I_{T} = I_{\Delta_1} + I_{\Delta_2} + \dots + I_{\Delta_p}
\end{equation*}

:::

## Principal axes and variance maximization

#### Finding the best axis (1)

Definition of the problem

- The best axis $\Delta_1$ is the "closest" to the point cloud
- Inertia of $\Delta_1$ measures the distance between the data and $\Delta_1$
- $\Delta_1$ is defined by the director vector $\bu_1$, such as $\| \bu_1 \| = 1$
- $\Delta_1^\bot$ is defined by the normal  vector $\bu_1$, such as $\| \bu_1 \| = 1$

\alert{$\rightsquigarrow$ The best axis $\Delta_1$ is the one with the minimal Inertia.}

## Finding the best axis (2)

#### Stating the optimization problem

Since $\Delta_1 \oplus \Delta_1^\bot = \Rset^p$ and $I_T = I_{\Delta_1} + I_{\Delta_1^\bot}$ , then

\begin{equation*}
  \minimize_{\bu \in \Rset^p: \|\bu\| = 1} I_{\Delta_1} \Leftrightarrow \maximize_{\bu \in \Rset^p: \|\bu\| = 1} I_{\Delta_1^\bot}
\end{equation*} 

![](figs_pca/minimum_inertia.jpg){with="60%"}

## Finding the best axis (3)

:::: {.columns}

::: {.column width="45%"}

#### Stating the problem (algebraically)

Find $\bu_1; \|\bu_1 \|=1$ that maximizes
\begin{equation*}
    \begin{aligned}
      I_{\Delta_1^\bot} & = \frac{1}{n}\sum_{i=1}^n \distance(\bx_i,\Delta_1^\bot)^2 \\ 
      & = \frac{1}{n}\sum_{i=1}^n \bu_1^\top (\bx_i - \bar{\bx})(\bx_i - \bar{\bx})^\top \bu_1 \\
      & = \bu_1^\top \left( \sum_{i=1}^n \frac{1}{n}(\bx_i - \bar{\bx})(\bx_i - \bar{\bx})^\top \right)  \bu_1 \\
      & = \bu_1^\top \hat{\bSigma}  \bu_1
    \end{aligned}
\end{equation*}

:::

::: {.column width="45%"}
  
![Geometrical insight](figs_pca/solving_inertia.jpg){width="80%"}

:::

::::

## Finding the best axis (4)

We solve a simple constraint maximization problem with the method of Lagrange multipliers:

\begin{equation*}
  \maximize_{\bu_1: \| \bu_1 \| = 1 } \bu_1^\top \hat{\bSigma} \bu_1 \Leftrightarrow \maximize_{\bu_1\in\Rset^p, \lambda_1 > 0} \bu_1^\top \hat{\bSigma} \bu_1 - \lambda_1 (\|\bu_1\|^2 - 1)
\end{equation*}

By straightforward (vector) differentiation, an using that $\bu_1^\top \bu_1 = 1$

\begin{equation*}
  \left\{\begin{aligned}
    2\hat{\bSigma} \bu_1 - 2\lambda_1 \bu_1 & = 0 \\
    \bu_1^\top \bu_1 - 1 & = 0 \\
  \end{aligned}\right. \Leftrightarrow
  \left\{\begin{aligned}
    \hat{\bSigma} \bu_1 & = \lambda_1 \bu_1  \\
    \bu_1^\top \hat{\bSigma} \bu_1 & = \lambda_1 \bu_1^\top \bu_1 = \lambda_1 = I_{\Delta_1}^\bot \\
  \end{aligned}\right.
\end{equation*}

  - $\bu_1$ is the first (normalized) eigen vector of $\hat{\bSigma}$
  - $\lambda_1$ is the first eigen value of $\hat{\bSigma}$

  $\rightsquigarrow $ \emphase{$\Delta_1$ is defined by the first eigen vector of $\hat{\bSigma}$ }

  $\rightsquigarrow $ \emphase{Variance "carried" by $\Delta_1$ is equal to the largest eigen value of $\hat{\bSigma}$ }

## Finding the following axes

#### Second best axis

Find $\Delta_2$ with dimension 1, director vector $\bu_2$ orthogonal to $\Delta_1$ solving

\begin{equation*}
    \maximize_{\bu_2 \in \Rset^p} I_{\Delta_2^\bot} = \bu_2^\top \hat{\bSigma}\bu_2, \quad \text{with } \|\bu_2\| = 1, \bu_1^\top \bu_2 = 0.
\end{equation*}

$\rightsquigarrow$ $\bu_2$ is the second eigen vector of $\hat{\bSigma}$ with eigen value $\lambda_2$

#### And so on!

PCA is roughly a matrix factorisation problem

\begin{equation*}
    \hat{\bSigma} = \bU {\boldsymbol\Lambda} \bU^\top, \quad
    \bU = \begin{pmatrix}
    \bu_1 & \bu_2, & \dots & \bu_p
    \end{pmatrix}, \quad {\boldsymbol\Lambda} = \diag(\lambda_1, \dots, \lambda_p)
\end{equation*}

\hspace{-.5cm}

  - $\bU$ is an orthogonal matrix of normalized eigen vectors.
  - ${\boldsymbol\Lambda}$ is diagonal matrix of  ordered eigen values.

## Interpretation in $\Rset^p$
    
$\bU$ describes a new orthogonal basis and a rotation of data in this basis
  
$\rightsquigarrow$ PCA is an appropriate rotation on axes that maximizes the variance

\begin{equation*}
  \left\{\begin{array}{ccccc}
    \Delta_1 & \oplus & \dots & \oplus & \Delta_p \\
    \bu_1 & \bot & \dots & \bot & \bu_p \\
    \lambda_1 & > & \dots & > & \lambda_p \\
    I_{\Delta_1^\bot} & > & \dots & > & I_{\Delta_p^\bot} \\
  \end{array}\right.
\end{equation*}

![](figs_pca/rotation.jpg){width="50%"}

## Unifying view of variables and individuals

#### Principal components

The full matrix of principal component connects  individual coordinates to latent factors:

\begin{equation*}
    \mathrm{PC} = \bX^c \bU = \begin{pmatrix}
    \mathbf{f}_{1} & \mathbf{f}_{2} & \dots & \mathbf{f}_{p}
    \end{pmatrix}
    = \begin{pmatrix} 
    \bc_{1}^\top \\ \bc_{2}^\top \\\dots \\ \bc_{n}^\top 
    \end{pmatrix}
\end{equation*}

\vfill
  
  - new variables (latent factor) are seen column-wise
  - new coordinates are seen row-wise

  $\rightsquigarrow$ Everything can be interpreted on a single plot, called the biplot

## Reconstruction formula

Recall that $\mathbf{F} = (\mathbf{f}_1, \dots, \mathbf{f}_p)$ is the matrix of Principal components. Then,

  - $\mathbf{f}_k = \bX^c \bu_k$ for projection on axis $k$
  - $\mathbf{F} = \bX^c \bU$ for all axis.

Using orthogonality of $\bU$, we get back the original data as follows, without loss ($\bU^T$ performs the inverse rotation of $\bU$):
\begin{equation*}
  \bX^c = \mathbf{F}\bU^\top
\end{equation*}

We obtain an approximation $\tilde{\bX}^c$ (compression) of the data $\bX^c$ by considering a subset $\mathcal{S}$ of PC, typically $\mathcal{S} = {1, \dots, q}$ with $q \ll p$.

\begin{equation*}
  \tilde{\bX}^c = \mathbf{F}_{\mathcal{S}}\bU_{\mathcal{S}}^\top = \bX^c \bU_{\mathcal{S}} \bU_{\mathcal{S}}^\top
\end{equation*}

$\rightsquigarrow$ This is a rank-$q$ approximation of $\bX$ (information captured by the first $q$ axes).
